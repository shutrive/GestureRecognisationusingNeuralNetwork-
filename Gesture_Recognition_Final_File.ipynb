{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRxVMr_IwZl3"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1594195691114,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "UsBq_OU5wZl5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import abc\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvhAuzfewZl9"
   },
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2774,
     "status": "ok",
     "timestamp": 1594195696895,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "XNomXwN2wZl_",
    "outputId": "bb8aa303-8d05-4b1c-b6ce-21507bf56b44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3159,
     "status": "ok",
     "timestamp": 1594195697446,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "U1ZD50aywZmC"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Kbnxps4wZmF"
   },
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1594195697447,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "55Xeh0aYwZmG"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZSeElmCwZmT"
   },
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1594195791100,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "kLHLEJruwZmT"
   },
   "outputs": [],
   "source": [
    "class ModelBuilder(metaclass= abc.ABCMeta):\n",
    "    \n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "\n",
    "        \n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "    \n",
    "    \n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7x1mfWIcwZmW"
   },
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gk5mDCATwZmX"
   },
   "source": [
    "## Sample Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1141,
     "status": "ok",
     "timestamp": 1594195798907,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "BoS9Sa6cwZmX"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam()\n",
    "        #optimiser = 'sgd'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10168,
     "status": "ok",
     "timestamp": 1594195808801,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "ukSIHweVwZma",
    "outputId": "84ebb5a0-dd1c-43f3-cda7-15a0db44ffd2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1638528   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,736,389\n",
      "Trainable params: 1,735,525\n",
      "Non-trainable params: 864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "conv_3d1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjtQQ58QwZmn"
   },
   "source": [
    "### We had hit the limit on memory resources with image resolution of 160x160 with 30 frames and batch_size of 40...we get the below error\n",
    "\n",
    "ResourceExhaustedError: OOM when allocating tensor with shape[40,16,30,160,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcLozfzKwZmr"
   },
   "source": [
    "##### So lets trade-off between these parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "wg_FEPTVwZms"
   },
   "source": [
    "##### Below are the experiments to see how training time is affected by image resolution, number of images in sequence and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5469,
     "status": "aborted",
     "timestamp": 1594195831453,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "_IUaP4jfwZmy"
   },
   "outputs": [],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4890,
     "status": "aborted",
     "timestamp": 1594195831455,
     "user": {
      "displayName": "Sayan Dey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0tYuahicdtu0ngYhiTohPNVHqEWKVjhIXb2sx=s64",
      "userId": "07932039372029331807"
     },
     "user_tz": -330
    },
    "id": "-C2UyfolwZm1"
   },
   "outputs": [],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=60,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byk_z0yDwZm6",
    "outputId": "3f5b764a-4b1b-43d0-f27f-c0b30e8d2df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 687813\n",
      "Epoch 1/2\n",
      "12/12 [==============================] - 50s 4s/step - loss: 1.6570 - categorical_accuracy: 0.3996 - val_loss: 1.4679 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_38_46.833789/model-00001-1.71621-0.37858-1.46788-0.51000.h5\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 36s 3s/step - loss: 1.2933 - categorical_accuracy: 0.4991 - val_loss: 1.3049 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_38_46.833789/model-00002-1.17992-0.53846-1.30490-0.45000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f685ece3d30>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=60,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUInhLtmwZm9",
    "outputId": "d15d58c0-47a4-48bc-b3fd-a8df7b56ed2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 687813\n",
      "Epoch 1/2\n",
      "9/9 [==============================] - 50s 6s/step - loss: 1.7385 - categorical_accuracy: 0.3560 - val_loss: 4.0362 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_40_25.251994/model-00001-1.77623-0.34992-4.03619-0.28000.h5\n",
      "Epoch 2/2\n",
      "9/9 [==============================] - 35s 4s/step - loss: 1.1918 - categorical_accuracy: 0.5322 - val_loss: 4.4454 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_40_25.251994/model-00002-1.13833-0.54299-4.44542-0.30000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f685d2e7d30>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=80,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYPl2k4DwZnB",
    "outputId": "31441f10-753d-4013-9eab-c29f5dcd9721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1736389\n",
      "Epoch 1/2\n",
      "45/45 [==============================] - 99s 2s/step - loss: 1.6126 - categorical_accuracy: 0.4088 - val_loss: 0.8895 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_42_04.666932/model-00001-1.62168-0.40422-0.88947-0.67000.h5\n",
      "Epoch 2/2\n",
      "45/45 [==============================] - 84s 2s/step - loss: 1.1739 - categorical_accuracy: 0.5379 - val_loss: 0.9146 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_42_04.666932/model-00002-1.17226-0.54148-0.91455-0.66000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f685bce5748>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=15,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66o4nFX-wZnG",
    "outputId": "ea825d14-869f-44be-f50a-b406b3ca5ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1736389\n",
      "Epoch 1/2\n",
      "45/45 [==============================] - 53s 1s/step - loss: 1.6318 - categorical_accuracy: 0.4061 - val_loss: 1.1419 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_45_19.479759/model-00001-1.62347-0.41327-1.14188-0.55000.h5\n",
      "Epoch 2/2\n",
      "45/45 [==============================] - 44s 983ms/step - loss: 1.0648 - categorical_accuracy: 0.5658 - val_loss: 1.1721 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_45_19.479759/model-00002-1.07743-0.55807-1.17206-0.60000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6859e31e10>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTyYRV7UwZnM",
    "outputId": "22e7bec9-7fe0-46dd-d307-ce48fe7a2704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 687813\n",
      "Epoch 1/2\n",
      "45/45 [==============================] - 43s 959ms/step - loss: 1.7223 - categorical_accuracy: 0.3556 - val_loss: 2.5921 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_47_07.828530/model-00001-1.72619-0.35596-2.59209-0.24000.h5\n",
      "Epoch 2/2\n",
      "45/45 [==============================] - 33s 737ms/step - loss: 1.2379 - categorical_accuracy: 0.5230 - val_loss: 2.1007 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_47_07.828530/model-00002-1.22910-0.52640-2.10070-0.36000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68583547b8>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDwxOKuXwZnQ",
    "outputId": "1d8f56c5-fd57-4a27-fbc7-09e30e7cdec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 687813\n",
      "Epoch 1/2\n",
      "67/67 [==============================] - 44s 650ms/step - loss: 1.5841 - categorical_accuracy: 0.4144 - val_loss: 1.8234 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_48_36.323543/model-00001-1.58654-0.41176-1.82336-0.43000.h5\n",
      "Epoch 2/2\n",
      "67/67 [==============================] - 34s 508ms/step - loss: 1.3479 - categorical_accuracy: 0.4596 - val_loss: 0.8982 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_48_36.323543/model-00002-1.35973-0.45400-0.89816-0.62000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6856549b38>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_ceXiXewZnU",
    "outputId": "d058aecc-c2bf-4c4d-fcf7-e5adc8ff90ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 687813\n",
      "Epoch 1/2\n",
      "67/67 [==============================] - 79s 1s/step - loss: 1.6217 - categorical_accuracy: 0.4323 - val_loss: 1.1418 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_50_06.750693/model-00001-1.62279-0.42986-1.14185-0.58000.h5\n",
      "Epoch 2/2\n",
      "67/67 [==============================] - 68s 1s/step - loss: 1.2239 - categorical_accuracy: 0.5299 - val_loss: 1.2135 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_50_06.750693/model-00002-1.20968-0.53544-1.21353-0.54000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f685a625fd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2FQVr39wZnY",
    "outputId": "cc1e9b3a-4e90-45c1-f154-9b6955815f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1736389\n",
      "Epoch 1/2\n",
      "67/67 [==============================] - 95s 1s/step - loss: 1.5623 - categorical_accuracy: 0.4069 - val_loss: 1.8928 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_52_46.202017/model-00001-1.56677-0.40422-1.89278-0.30000.h5\n",
      "Epoch 2/2\n",
      "67/67 [==============================] - 79s 1s/step - loss: 1.2894 - categorical_accuracy: 0.5219 - val_loss: 0.9455 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_52_46.202017/model-00002-1.28746-0.52036-0.94555-0.66000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6852d03cf8>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HE37Xo9UwZnf",
    "outputId": "329d6944-45b8-424d-a926-fc811f02ed59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1736389\n",
      "Epoch 1/2\n",
      "67/67 [==============================] - 53s 790ms/step - loss: 1.6596 - categorical_accuracy: 0.3677 - val_loss: 1.1056 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1614_58_22.784838/model-00001-1.66112-0.36802-1.10557-0.55000.h5\n",
      "Epoch 2/2\n",
      "67/67 [==============================] - 42s 630ms/step - loss: 1.3546 - categorical_accuracy: 0.4696 - val_loss: 1.0475 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1614_58_22.784838/model-00002-1.35939-0.46757-1.04752-0.58000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f685170e7b8>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNdr_EjjwZni",
    "outputId": "8a7a4fac-620a-4e9b-e3a1-55b7edc3988a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1736389\n",
      "Epoch 1/2\n",
      "17/17 [==============================] - 60s 4s/step - loss: 1.6747 - categorical_accuracy: 0.3830 - val_loss: 0.9994 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1615_00_11.348675/model-00001-1.68751-0.37858-0.99945-0.58000.h5\n",
      "Epoch 2/2\n",
      "17/17 [==============================] - 35s 2s/step - loss: 1.0526 - categorical_accuracy: 0.5903 - val_loss: 0.8694 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1615_00_11.348675/model-00002-1.05775-0.58673-0.86944-0.66000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f684f7d3a20>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=40,num_epochs=2)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AYG4fq8wZnm"
   },
   "source": [
    "##### As we see from the above experiments image resolution and number of frames in sequence have more impact on training time than batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "qiyBE3EywZnn"
   },
   "source": [
    "##### So experimentations are carried with batch size fixed around 15-40 and changing the resolution and number of image per sequence based on the device memory constraints . Models are designed such that their memory foot print is less than 50 MB which corresponds to 4.3 million parameters assuming the datatype size of parameters to be 12 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRhBolLjwZno"
   },
   "source": [
    "# Model 1 - Base Model - No Data Augmentation Batch Size 40 and Epoch 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N4ldRY-vwZnp"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_train_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6a0051279329>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_train_sequences\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train_sequences\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_train_sequences\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_train_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=2, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bay1T7ZTwZnt",
    "outputId": "7edb343d-3342-4da5-a4bc-7fcef5c76ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_49 (Conv3D)           (None, 20, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 20, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 20, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_49 (MaxPooling (None, 10, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_50 (Conv3D)           (None, 10, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 10, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 10, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_50 (MaxPooling (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_51 (Conv3D)           (None, 5, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 5, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_51 (MaxPooling (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_52 (Conv3D)           (None, 2, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 2, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 2, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_52 (MaxPooling (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                819264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,117,061\n",
      "Trainable params: 1,116,325\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=20,batch_size=40,num_epochs=15)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "conv_3d1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQbLCm--wZny",
    "outputId": "36afecad-2b44-46c6-a07e-c4b71a6dcd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1117061\n",
      "Epoch 1/15\n",
      "17/17 [==============================] - 72s 4s/step - loss: 1.6484 - categorical_accuracy: 0.3793 - val_loss: 3.5821 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1007_29_09.439229/model-00001-1.65039-0.38009-3.58210-0.28000.h5\n",
      "Epoch 2/15\n",
      "17/17 [==============================] - 52s 3s/step - loss: 1.0203 - categorical_accuracy: 0.6001 - val_loss: 1.3206 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1007_29_09.439229/model-00002-1.01788-0.59879-1.32063-0.55000.h5\n",
      "Epoch 3/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.7766 - categorical_accuracy: 0.7011 - val_loss: 0.8526 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1007_29_09.439229/model-00003-0.77347-0.70437-0.85258-0.65000.h5\n",
      "Epoch 4/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.5859 - categorical_accuracy: 0.7809 - val_loss: 0.9037 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1007_29_09.439229/model-00004-0.58786-0.77979-0.90373-0.63000.h5\n",
      "Epoch 5/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.4478 - categorical_accuracy: 0.8388 - val_loss: 1.1431 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1007_29_09.439229/model-00005-0.44629-0.84012-1.14308-0.63000.h5\n",
      "Epoch 6/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.2981 - categorical_accuracy: 0.9140 - val_loss: 0.9457 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1007_29_09.439229/model-00006-0.29899-0.91403-0.94571-0.61000.h5\n",
      "Epoch 7/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.2030 - categorical_accuracy: 0.9493 - val_loss: 1.2352 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1007_29_09.439229/model-00007-0.20247-0.95023-1.23525-0.54000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/15\n",
      "17/17 [==============================] - 59s 3s/step - loss: 0.1728 - categorical_accuracy: 0.9582 - val_loss: 0.9122 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1007_29_09.439229/model-00008-0.17014-0.95928-0.91221-0.63000.h5\n",
      "Epoch 9/15\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.1207 - categorical_accuracy: 0.9794 - val_loss: 0.7586 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1007_29_09.439229/model-00009-0.12198-0.97888-0.75856-0.73000.h5\n",
      "Epoch 10/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.1037 - categorical_accuracy: 0.9897 - val_loss: 0.7030 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1007_29_09.439229/model-00010-0.10324-0.98944-0.70304-0.74000.h5\n",
      "Epoch 11/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.0935 - categorical_accuracy: 0.9916 - val_loss: 0.6229 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1007_29_09.439229/model-00011-0.09234-0.99246-0.62294-0.76000.h5\n",
      "Epoch 12/15\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.0970 - categorical_accuracy: 0.9832 - val_loss: 0.7111 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1007_29_09.439229/model-00012-0.09341-0.98492-0.71109-0.77000.h5\n",
      "Epoch 13/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.0780 - categorical_accuracy: 0.9897 - val_loss: 0.6646 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1007_29_09.439229/model-00013-0.07871-0.98944-0.66458-0.77000.h5\n",
      "Epoch 14/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.0775 - categorical_accuracy: 0.9941 - val_loss: 0.6503 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1007_29_09.439229/model-00014-0.07753-0.99397-0.65028-0.78000.h5\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.0698 - categorical_accuracy: 0.9926 - val_loss: 0.6544 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1007_29_09.439229/model-00015-0.06922-0.99246-0.65437-0.77000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "history_model1 = conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fosbi1WWwZn3"
   },
   "source": [
    "##### Model is clearly overfitting. So we need to do data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDHlmpUpwZn3"
   },
   "source": [
    "# Model 2 - Augment Data , (3,3,3) filter & 160x160 image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yae6Jk57wZn4",
    "outputId": "d080fa60-adc2-4adc-d2c2-2b7563b8d186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_37 (Conv3D)           (None, 20, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 20, 160, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 20, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_37 (MaxPooling (None, 10, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 10, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 10, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 10, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_38 (MaxPooling (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 5, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 5, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 5, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_39 (MaxPooling (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 2, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 2, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 2, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_40 (MaxPooling (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 3,638,981\n",
      "Trainable params: 3,637,477\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d2=ModelConv3D1()\n",
    "conv_3d2.initialize_path(project_folder)\n",
    "conv_3d2.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=25)\n",
    "conv_3d2_model=conv_3d2.define_model(dense_neurons=256,dropout=0.5)\n",
    "conv_3d2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xenwDMxUwZn6",
    "outputId": "4d13fe15-91a5-4cf7-d334-e95129340caf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3638981\n",
      "Epoch 1/25\n",
      "34/34 [==============================] - 145s 4s/step - loss: 1.9434 - categorical_accuracy: 0.3988 - val_loss: 1.3561 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1005_42_21.065484/model-00001-1.94407-0.40045-1.35607-0.55000.h5\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - 128s 4s/step - loss: 1.4510 - categorical_accuracy: 0.5247 - val_loss: 2.0266 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1005_42_21.065484/model-00002-1.44054-0.52112-2.02662-0.47000.h5\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 1.1826 - categorical_accuracy: 0.5861 - val_loss: 1.1703 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1005_42_21.065484/model-00003-1.16885-0.58824-1.17029-0.67000.h5\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - 135s 4s/step - loss: 1.0443 - categorical_accuracy: 0.6181 - val_loss: 0.8861 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1005_42_21.065484/model-00004-1.05186-0.61689-0.88614-0.66000.h5\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - 135s 4s/step - loss: 0.9523 - categorical_accuracy: 0.6556 - val_loss: 1.0979 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1005_42_21.065484/model-00005-0.91074-0.66365-1.09794-0.66000.h5\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.8717 - categorical_accuracy: 0.6810 - val_loss: 0.9429 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1005_42_21.065484/model-00006-0.88580-0.67722-0.94290-0.66000.h5\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.6333 - categorical_accuracy: 0.7572 - val_loss: 1.2574 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1005_42_21.065484/model-00007-0.64395-0.75113-1.25740-0.58000.h5\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - 135s 4s/step - loss: 0.6480 - categorical_accuracy: 0.7745 - val_loss: 1.4248 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1005_42_21.065484/model-00008-0.63117-0.77300-1.42484-0.55000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.5312 - categorical_accuracy: 0.7999 - val_loss: 1.3346 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1005_42_21.065484/model-00009-0.48784-0.81146-1.33461-0.55000.h5\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.4268 - categorical_accuracy: 0.8451 - val_loss: 0.8712 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1005_42_21.065484/model-00010-0.43060-0.84540-0.87124-0.69000.h5\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.4024 - categorical_accuracy: 0.8414 - val_loss: 0.9785 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1005_42_21.065484/model-00011-0.40134-0.84163-0.97848-0.69000.h5\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.3876 - categorical_accuracy: 0.8400 - val_loss: 0.6610 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1005_42_21.065484/model-00012-0.39207-0.84012-0.66101-0.76000.h5\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.3723 - categorical_accuracy: 0.8631 - val_loss: 0.7904 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1005_42_21.065484/model-00013-0.36647-0.86802-0.79036-0.67000.h5\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.3886 - categorical_accuracy: 0.8675 - val_loss: 0.6045 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1005_42_21.065484/model-00014-0.35090-0.88084-0.60449-0.75000.h5\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.3750 - categorical_accuracy: 0.8650 - val_loss: 0.7360 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1005_42_21.065484/model-00015-0.33305-0.87406-0.73603-0.74000.h5\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.3443 - categorical_accuracy: 0.8760 - val_loss: 0.7860 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1005_42_21.065484/model-00016-0.34564-0.87707-0.78598-0.77000.h5\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - 135s 4s/step - loss: 0.3699 - categorical_accuracy: 0.8760 - val_loss: 0.8258 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1005_42_21.065484/model-00017-0.31201-0.88537-0.82578-0.69000.h5\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.2821 - categorical_accuracy: 0.8955 - val_loss: 0.6994 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1005_42_21.065484/model-00018-0.28500-0.89291-0.69942-0.74000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.3064 - categorical_accuracy: 0.9113 - val_loss: 0.6875 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1005_42_21.065484/model-00019-0.24858-0.92157-0.68751-0.73000.h5\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.2585 - categorical_accuracy: 0.9047 - val_loss: 0.6122 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1005_42_21.065484/model-00020-0.25789-0.90649-0.61217-0.77000.h5\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.3184 - categorical_accuracy: 0.8981 - val_loss: 0.5808 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00021: saving model to model_init_2019-03-1005_42_21.065484/model-00021-0.24467-0.91629-0.58076-0.82000.h5\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.2078 - categorical_accuracy: 0.9260 - val_loss: 0.5413 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00022: saving model to model_init_2019-03-1005_42_21.065484/model-00022-0.20651-0.92836-0.54127-0.83000.h5\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - 135s 4s/step - loss: 0.2162 - categorical_accuracy: 0.9139 - val_loss: 0.5374 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00023: saving model to model_init_2019-03-1005_42_21.065484/model-00023-0.20454-0.92836-0.53737-0.85000.h5\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - 136s 4s/step - loss: 0.2248 - categorical_accuracy: 0.9165 - val_loss: 0.5726 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00024: saving model to model_init_2019-03-1005_42_21.065484/model-00024-0.21627-0.91855-0.57263-0.80000.h5\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - 136s 4s/step - loss: 0.2718 - categorical_accuracy: 0.9091 - val_loss: 0.5287 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00025: saving model to model_init_2019-03-1005_42_21.065484/model-00025-0.23324-0.91931-0.52868-0.83000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d2_model.count_params())\n",
    "history_model2=conv_3d2.train_model(conv_3d2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfr0DfqqwZn-"
   },
   "source": [
    "##### Model is not overfitting and we get a best validation accuracy of 85% and training accuracy of 91%. Next we will try to reduce the filter size and image resolution and see if get better results. Moreover since we see minor oscillations  in loss, let's try lowering the learning rate to 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTBxoG8-wZn_"
   },
   "source": [
    "# Model 3 - Reduce filter size to (2,2,2) and image res to 120 x  120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZIgPhHqwwZn_"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D3(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc0sQkSKwZoC",
    "outputId": "1e0c9058-ebe8-4c2e-ad0d-41d9d36d6fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_25 (Conv3D)           (None, 16, 120, 120, 16)  400       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 8, 60, 60, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 8, 60, 60, 32)     4128      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 4, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 4, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,762,613\n",
      "Trainable params: 1,761,109\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d3=ModelConv3D3()\n",
    "conv_3d3.initialize_path(project_folder)\n",
    "conv_3d3.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n",
    "conv_3d3_model=conv_3d3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
    "conv_3d3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nvu4JJWnwZoF",
    "outputId": "8bb1509e-c13b-430e-aeba-8bc4138aadb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1762613\n",
      "Epoch 1/30\n",
      "23/23 [==============================] - 97s 4s/step - loss: 2.1588 - categorical_accuracy: 0.3378 - val_loss: 1.2613 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1115_20_32.906637/model-00001-2.18973-0.32504-1.26131-0.60000.h5\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - 86s 4s/step - loss: 1.5574 - categorical_accuracy: 0.4451 - val_loss: 1.0693 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1115_20_32.906637/model-00002-1.54651-0.44947-1.06930-0.62000.h5\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.4757 - categorical_accuracy: 0.4930 - val_loss: 0.8723 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1115_20_32.906637/model-00003-1.43319-0.49925-0.87225-0.67000.h5\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.2460 - categorical_accuracy: 0.5775 - val_loss: 0.7799 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1115_20_32.906637/model-00004-1.15351-0.59351-0.77988-0.71000.h5\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.0809 - categorical_accuracy: 0.6196 - val_loss: 0.7716 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1115_20_32.906637/model-00005-1.03587-0.63725-0.77164-0.73000.h5\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.9948 - categorical_accuracy: 0.6222 - val_loss: 0.7243 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1115_20_32.906637/model-00006-0.98321-0.63348-0.72432-0.76000.h5\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.9356 - categorical_accuracy: 0.6601 - val_loss: 0.8617 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1115_20_32.906637/model-00007-0.93103-0.65988-0.86166-0.74000.h5\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.7912 - categorical_accuracy: 0.7089 - val_loss: 0.7941 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1115_20_32.906637/model-00008-0.81848-0.69759-0.79409-0.77000.h5\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.7845 - categorical_accuracy: 0.7076 - val_loss: 0.8169 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1115_20_32.906637/model-00009-0.78104-0.71569-0.81689-0.76000.h5\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.7326 - categorical_accuracy: 0.7337 - val_loss: 0.7013 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1115_20_32.906637/model-00010-0.71093-0.74284-0.70131-0.79000.h5\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.7161 - categorical_accuracy: 0.7318 - val_loss: 0.6978 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1115_20_32.906637/model-00011-0.66456-0.74736-0.69779-0.79000.h5\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.7030 - categorical_accuracy: 0.7522 - val_loss: 0.9487 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1115_20_32.906637/model-00012-0.63976-0.76848-0.94874-0.71000.h5\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.6946 - categorical_accuracy: 0.7632 - val_loss: 0.9414 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1115_20_32.906637/model-00013-0.64772-0.76697-0.94142-0.71000.h5\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6126 - categorical_accuracy: 0.7514 - val_loss: 0.8390 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1115_20_32.906637/model-00014-0.59759-0.76772-0.83901-0.76000.h5\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6004 - categorical_accuracy: 0.7943 - val_loss: 0.7423 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1115_20_32.906637/model-00015-0.51161-0.81222-0.74227-0.74000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.6687 - categorical_accuracy: 0.7749 - val_loss: 0.7062 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1115_20_32.906637/model-00016-0.55235-0.79864-0.70621-0.77000.h5\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6059 - categorical_accuracy: 0.7848 - val_loss: 0.6748 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1115_20_32.906637/model-00017-0.56529-0.80241-0.67476-0.78000.h5\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5402 - categorical_accuracy: 0.7925 - val_loss: 0.6561 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1115_20_32.906637/model-00018-0.51415-0.80392-0.65608-0.79000.h5\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4969 - categorical_accuracy: 0.8123 - val_loss: 0.6378 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1115_20_32.906637/model-00019-0.49243-0.81146-0.63779-0.79000.h5\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5091 - categorical_accuracy: 0.8176 - val_loss: 0.6300 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1115_20_32.906637/model-00020-0.47233-0.82353-0.62997-0.82000.h5\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4576 - categorical_accuracy: 0.8136 - val_loss: 0.6270 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00021: saving model to model_init_2019-03-1115_20_32.906637/model-00021-0.45393-0.82579-0.62695-0.83000.h5\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.5218 - categorical_accuracy: 0.8040 - val_loss: 0.6320 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00022: saving model to model_init_2019-03-1115_20_32.906637/model-00022-0.45554-0.82881-0.63204-0.81000.h5\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5733 - categorical_accuracy: 0.8033 - val_loss: 0.6297 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00023: saving model to model_init_2019-03-1115_20_32.906637/model-00023-0.50425-0.82805-0.62974-0.81000.h5\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4884 - categorical_accuracy: 0.8092 - val_loss: 0.6543 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00024: saving model to model_init_2019-03-1115_20_32.906637/model-00024-0.48211-0.82127-0.65434-0.82000.h5\n",
      "Epoch 25/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4868 - categorical_accuracy: 0.8245 - val_loss: 0.6688 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00025: saving model to model_init_2019-03-1115_20_32.906637/model-00025-0.44564-0.83710-0.66877-0.85000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "Epoch 26/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.5340 - categorical_accuracy: 0.8139 - val_loss: 0.6630 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00026: saving model to model_init_2019-03-1115_20_32.906637/model-00026-0.44371-0.83258-0.66304-0.84000.h5\n",
      "Epoch 27/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4802 - categorical_accuracy: 0.8208 - val_loss: 0.6577 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00027: saving model to model_init_2019-03-1115_20_32.906637/model-00027-0.44857-0.83333-0.65767-0.83000.h5\n",
      "Epoch 28/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4596 - categorical_accuracy: 0.8266 - val_loss: 0.6554 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00028: saving model to model_init_2019-03-1115_20_32.906637/model-00028-0.44158-0.83937-0.65541-0.82000.h5\n",
      "Epoch 29/30\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4913 - categorical_accuracy: 0.8165 - val_loss: 0.6496 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00029: saving model to model_init_2019-03-1115_20_32.906637/model-00029-0.46451-0.82881-0.64963-0.82000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.5999999959603884e-06.\n",
      "Epoch 30/30\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4327 - categorical_accuracy: 0.8428 - val_loss: 0.6439 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00030: saving model to model_init_2019-03-1115_20_32.906637/model-00030-0.41932-0.84314-0.64393-0.83000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d3_model.count_params())\n",
    "history_model3=conv_3d3.train_model(conv_3d3_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNEpMjY5wZoL"
   },
   "source": [
    "##### Model has a  best validation accuracy of 84% and training accuracy of 84% . Also we were able to reduce the parameter size by half the earlier model. Let's trying adding more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHDiPS2SwZoL"
   },
   "source": [
    "# Model 4 - Adding more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hGei4KnYwZoM"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D4(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnkZpuTAwZoP",
    "outputId": "56f586d5-d71e-4299-e570-f937f157f284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 16, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 16, 120, 120, 16)  6928      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 8, 60, 60, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 8, 60, 60, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 8, 60, 60, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 4, 30, 30, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 4, 30, 30, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 2, 15, 15, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 2, 15, 15, 128)    442496    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,556,533\n",
      "Trainable params: 2,554,549\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d4=ModelConv3D4()\n",
    "conv_3d4.initialize_path(project_folder)\n",
    "conv_3d4.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d4.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
    "conv_3d4_model=conv_3d4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
    "conv_3d4_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7ywqUVkwZoW",
    "outputId": "fcef8d72-1ff2-44a9-96b6-032615e40384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2556533\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - 209s 6s/step - loss: 2.0704 - categorical_accuracy: 0.3400 - val_loss: 3.7693 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1008_46_09.184617/model-00001-2.03959-0.34012-3.76934-0.36000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 97s 3s/step - loss: 1.5526 - categorical_accuracy: 0.4724 - val_loss: 3.8769 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1008_46_09.184617/model-00002-1.50234-0.48416-3.87689-0.36000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 1.3620 - categorical_accuracy: 0.5364 - val_loss: 2.0781 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1008_46_09.184617/model-00003-1.37034-0.53318-2.07815-0.49000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 1.2027 - categorical_accuracy: 0.5809 - val_loss: 2.5479 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1008_46_09.184617/model-00004-1.19968-0.58296-2.54788-0.46000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 1.1011 - categorical_accuracy: 0.6089 - val_loss: 1.5326 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1008_46_09.184617/model-00005-1.08681-0.61161-1.53259-0.56000.h5\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 105s 3s/step - loss: 1.1071 - categorical_accuracy: 0.6052 - val_loss: 6.0839 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1008_46_09.184617/model-00006-1.09813-0.60784-6.08394-0.36000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 103s 3s/step - loss: 1.0829 - categorical_accuracy: 0.6078 - val_loss: 1.2969 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1008_46_09.184617/model-00007-1.06288-0.60633-1.29687-0.62000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.8935 - categorical_accuracy: 0.6873 - val_loss: 1.1943 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1008_46_09.184617/model-00008-0.81299-0.70437-1.19432-0.67000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.8370 - categorical_accuracy: 0.6928 - val_loss: 0.7457 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1008_46_09.184617/model-00009-0.83254-0.68929-0.74570-0.70000.h5\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 103s 3s/step - loss: 0.7640 - categorical_accuracy: 0.7244 - val_loss: 0.8600 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1008_46_09.184617/model-00010-0.74205-0.73002-0.86004-0.71000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 104s 3s/step - loss: 0.6999 - categorical_accuracy: 0.7380 - val_loss: 0.9880 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1008_46_09.184617/model-00011-0.69752-0.73982-0.98796-0.61000.h5\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 103s 3s/step - loss: 0.6781 - categorical_accuracy: 0.7395 - val_loss: 0.8505 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1008_46_09.184617/model-00012-0.66812-0.74133-0.85048-0.72000.h5\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.6529 - categorical_accuracy: 0.7432 - val_loss: 0.9283 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1008_46_09.184617/model-00013-0.63107-0.75339-0.92829-0.71000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 103s 3s/step - loss: 0.6219 - categorical_accuracy: 0.7697 - val_loss: 0.7178 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1008_46_09.184617/model-00014-0.57985-0.78054-0.71776-0.75000.h5\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 104s 3s/step - loss: 0.4336 - categorical_accuracy: 0.8256 - val_loss: 0.5587 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1008_46_09.184617/model-00015-0.44326-0.82127-0.55874-0.77000.h5\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 103s 3s/step - loss: 0.4728 - categorical_accuracy: 0.8330 - val_loss: 0.6148 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1008_46_09.184617/model-00016-0.45704-0.83710-0.61481-0.71000.h5\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 104s 3s/step - loss: 0.5276 - categorical_accuracy: 0.8068 - val_loss: 0.6514 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1008_46_09.184617/model-00017-0.47431-0.81448-0.65139-0.77000.h5\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 103s 3s/step - loss: 0.4481 - categorical_accuracy: 0.8197 - val_loss: 0.6282 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1008_46_09.184617/model-00018-0.43068-0.83183-0.62823-0.76000.h5\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 101s 3s/step - loss: 0.4473 - categorical_accuracy: 0.8565 - val_loss: 0.6538 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1008_46_09.184617/model-00019-0.38896-0.86124-0.65380-0.75000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 101s 3s/step - loss: 0.3499 - categorical_accuracy: 0.8528 - val_loss: 0.6286 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1008_46_09.184617/model-00020-0.33046-0.86576-0.62861-0.74000.h5\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.3319 - categorical_accuracy: 0.8782 - val_loss: 0.6297 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00021: saving model to model_init_2019-03-1008_46_09.184617/model-00021-0.30804-0.88763-0.62974-0.76000.h5\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 101s 3s/step - loss: 0.3208 - categorical_accuracy: 0.8698 - val_loss: 0.6183 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00022: saving model to model_init_2019-03-1008_46_09.184617/model-00022-0.29581-0.88311-0.61833-0.75000.h5\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.3373 - categorical_accuracy: 0.8698 - val_loss: 0.6178 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00023: saving model to model_init_2019-03-1008_46_09.184617/model-00023-0.32649-0.87481-0.61781-0.77000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 101s 3s/step - loss: 0.3691 - categorical_accuracy: 0.8591 - val_loss: 0.6095 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00024: saving model to model_init_2019-03-1008_46_09.184617/model-00024-0.37139-0.85973-0.60951-0.75000.h5\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.3235 - categorical_accuracy: 0.8996 - val_loss: 0.6105 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00025: saving model to model_init_2019-03-1008_46_09.184617/model-00025-0.27871-0.90950-0.61048-0.75000.h5\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.2994 - categorical_accuracy: 0.8959 - val_loss: 0.6083 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00026: saving model to model_init_2019-03-1008_46_09.184617/model-00026-0.29811-0.89744-0.60827-0.76000.h5\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 101s 3s/step - loss: 0.2984 - categorical_accuracy: 0.8867 - val_loss: 0.6149 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00027: saving model to model_init_2019-03-1008_46_09.184617/model-00027-0.28661-0.89216-0.61490-0.75000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.3109 - categorical_accuracy: 0.8874 - val_loss: 0.6103 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00028: saving model to model_init_2019-03-1008_46_09.184617/model-00028-0.29596-0.89291-0.61028-0.76000.h5\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 102s 3s/step - loss: 0.3224 - categorical_accuracy: 0.8859 - val_loss: 0.6085 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00029: saving model to model_init_2019-03-1008_46_09.184617/model-00029-0.28613-0.89140-0.60855-0.76000.h5\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 103s 3s/step - loss: 0.3095 - categorical_accuracy: 0.8900 - val_loss: 0.6079 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00030: saving model to model_init_2019-03-1008_46_09.184617/model-00030-0.30487-0.89140-0.60794-0.76000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d4_model.count_params())\n",
    "history_model4=conv_3d4.train_model(conv_3d4_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWPtLsZ-wZof"
   },
   "source": [
    "##### With more layers we dont see much performance improvement. We get a best validation accuracy of 76% . Let's try adding dropouts at the convolution layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyjVsjYewZoh"
   },
   "source": [
    "# Model 5 Adding dropout at convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nQex2WNXwZoh"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D5(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdfEhePnwZoj",
    "outputId": "92b1b514-251c-4e20-e7f9-728b754898a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_33 (Conv3D)           (None, 16, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "conv3d_34 (Conv3D)           (None, 16, 120, 120, 16)  6928      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 8, 60, 60, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 8, 60, 60, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 8, 60, 60, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 4, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 4, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 4, 30, 30, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 4, 30, 30, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 2, 15, 15, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 2, 15, 15, 128)    442496    \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,556,533\n",
      "Trainable params: 2,554,549\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d5=ModelConv3D5()\n",
    "conv_3d5.initialize_path(project_folder)\n",
    "conv_3d5.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d5.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=22)\n",
    "conv_3d5_model=conv_3d5.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n",
    "conv_3d5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZEhhkE3wZom",
    "outputId": "96fd2d8a-7fe1-463a-cc7a-1ff4adffc6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2556533\n",
      "Epoch 1/22\n",
      "34/34 [==============================] - 112s 3s/step - loss: 1.7839 - categorical_accuracy: 0.3775 - val_loss: 5.3131 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1012_55_02.718281/model-00001-1.80713-0.37029-5.31309-0.33000.h5\n",
      "Epoch 2/22\n",
      "34/34 [==============================] - 92s 3s/step - loss: 1.2731 - categorical_accuracy: 0.5258 - val_loss: 2.0889 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1012_55_02.718281/model-00002-1.28196-0.51810-2.08886-0.53000.h5\n",
      "Epoch 3/22\n",
      "34/34 [==============================] - 93s 3s/step - loss: 1.1038 - categorical_accuracy: 0.5806 - val_loss: 3.7566 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1012_55_02.718281/model-00003-1.11794-0.57843-3.75659-0.37000.h5\n",
      "Epoch 4/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 1.0271 - categorical_accuracy: 0.6015 - val_loss: 3.0447 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1012_55_02.718281/model-00004-0.99096-0.61237-3.04471-0.39000.h5\n",
      "Epoch 5/22\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.9280 - categorical_accuracy: 0.6475 - val_loss: 1.2107 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1012_55_02.718281/model-00005-0.90301-0.65535-1.21067-0.61000.h5\n",
      "Epoch 6/22\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.8714 - categorical_accuracy: 0.6667 - val_loss: 2.7568 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1012_55_02.718281/model-00006-0.85766-0.67496-2.75678-0.42000.h5\n",
      "Epoch 7/22\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.7826 - categorical_accuracy: 0.7167 - val_loss: 1.9708 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1012_55_02.718281/model-00007-0.74783-0.71795-1.97076-0.49000.h5\n",
      "Epoch 8/22\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.7487 - categorical_accuracy: 0.7167 - val_loss: 2.2404 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1012_55_02.718281/model-00008-0.70960-0.72624-2.24035-0.42000.h5\n",
      "Epoch 9/22\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.6124 - categorical_accuracy: 0.7693 - val_loss: 1.4670 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1012_55_02.718281/model-00009-0.57417-0.78431-1.46701-0.57000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.6136 - categorical_accuracy: 0.7612 - val_loss: 1.0190 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1012_55_02.718281/model-00010-0.61632-0.75943-1.01904-0.64000.h5\n",
      "Epoch 11/22\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.5002 - categorical_accuracy: 0.8201 - val_loss: 0.7758 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1012_55_02.718281/model-00011-0.46038-0.82805-0.77584-0.67000.h5\n",
      "Epoch 12/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.3866 - categorical_accuracy: 0.8558 - val_loss: 0.9371 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1012_55_02.718281/model-00012-0.36454-0.86048-0.93707-0.65000.h5\n",
      "Epoch 13/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.3916 - categorical_accuracy: 0.8528 - val_loss: 1.1792 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1012_55_02.718281/model-00013-0.38585-0.85747-1.17919-0.63000.h5\n",
      "Epoch 14/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.3320 - categorical_accuracy: 0.8940 - val_loss: 1.1191 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1012_55_02.718281/model-00014-0.30926-0.89970-1.11908-0.57000.h5\n",
      "Epoch 15/22\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.3155 - categorical_accuracy: 0.8823 - val_loss: 1.1402 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1012_55_02.718281/model-00015-0.32134-0.87934-1.14018-0.61000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 16/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.3054 - categorical_accuracy: 0.8944 - val_loss: 1.0649 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1012_55_02.718281/model-00016-0.27241-0.90422-1.06488-0.61000.h5\n",
      "Epoch 17/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.2584 - categorical_accuracy: 0.9205 - val_loss: 0.9886 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1012_55_02.718281/model-00017-0.25644-0.91855-0.98856-0.66000.h5\n",
      "Epoch 18/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.3210 - categorical_accuracy: 0.8907 - val_loss: 0.9295 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1012_55_02.718281/model-00018-0.28304-0.90045-0.92945-0.70000.h5\n",
      "Epoch 19/22\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.2499 - categorical_accuracy: 0.9069 - val_loss: 0.8964 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1012_55_02.718281/model-00019-0.24843-0.90875-0.89636-0.67000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 20/22\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.2688 - categorical_accuracy: 0.9095 - val_loss: 0.8669 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1012_55_02.718281/model-00020-0.22944-0.92383-0.86695-0.69000.h5\n",
      "Epoch 21/22\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.2768 - categorical_accuracy: 0.9025 - val_loss: 0.8668 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00021: saving model to model_init_2019-03-1012_55_02.718281/model-00021-0.26971-0.90422-0.86681-0.69000.h5\n",
      "Epoch 22/22\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.2607 - categorical_accuracy: 0.9117 - val_loss: 0.8519 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00022: saving model to model_init_2019-03-1012_55_02.718281/model-00022-0.25035-0.91780-0.85189-0.69000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d5_model.count_params())\n",
    "history_model5=conv_3d5.train_model(conv_3d5_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "ocmrhO6TwZow"
   },
   "source": [
    "##### Adding dropouts has further reduced validation accuracy as its not to learn  generalizable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx1piB8wZow"
   },
   "source": [
    "##### All models experimental models above have more than 1 million parameters. Let's try to reduce the model size and see the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_nmjx2NwZox"
   },
   "source": [
    "# Model 6 - reducing the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Mf6YAcaqwZox"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D6(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jUGUJvCwZo1",
    "outputId": "82ba1fcc-2b38-41c3-d92e-cc298030ee18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_65 (Conv3D)           (None, 16, 100, 100, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 16, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 16, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_45 (MaxPooling (None, 8, 50, 50, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_66 (Conv3D)           (None, 8, 50, 50, 32)     4128      \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 8, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 8, 50, 50, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_46 (MaxPooling (None, 4, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_67 (Conv3D)           (None, 4, 25, 25, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 4, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 4, 25, 25, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_47 (MaxPooling (None, 2, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_68 (Conv3D)           (None, 2, 12, 12, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 2, 12, 12, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 2, 12, 12, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_48 (MaxPooling (None, 1, 6, 6, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 696,645\n",
      "Trainable params: 695,653\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d6=ModelConv3D6()\n",
    "conv_3d6.initialize_path(project_folder)\n",
    "conv_3d6.initialize_image_properties(image_height=100,image_width=100)\n",
    "conv_3d6.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
    "conv_3d6_model=conv_3d6.define_model(dense_neurons=128,dropout=0.25)\n",
    "conv_3d6_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsCVqhZMwZo7",
    "outputId": "fade4ca6-07fa-40d6-e823-8a2fc4c8d5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 696645\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - 85s 2s/step - loss: 1.8024 - categorical_accuracy: 0.3304 - val_loss: 1.2390 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1014_58_54.727215/model-00001-1.80901-0.33032-1.23904-0.52000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 73s 2s/step - loss: 1.2119 - categorical_accuracy: 0.5375 - val_loss: 1.0104 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1014_58_54.727215/model-00002-1.17422-0.54676-1.01044-0.63000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.9485 - categorical_accuracy: 0.6380 - val_loss: 0.8189 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1014_58_54.727215/model-00003-0.94836-0.63725-0.81888-0.69000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.8501 - categorical_accuracy: 0.6560 - val_loss: 0.9353 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1014_58_54.727215/model-00004-0.83318-0.65988-0.93525-0.62000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 79s 2s/step - loss: 0.7741 - categorical_accuracy: 0.7060 - val_loss: 0.8962 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1014_58_54.727215/model-00005-0.77555-0.70287-0.89619-0.68000.h5\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.6844 - categorical_accuracy: 0.7432 - val_loss: 0.7620 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1014_58_54.727215/model-00006-0.69678-0.73680-0.76202-0.71000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 81s 2s/step - loss: 0.6449 - categorical_accuracy: 0.7583 - val_loss: 0.7596 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1014_58_54.727215/model-00007-0.63275-0.76471-0.75964-0.70000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.5620 - categorical_accuracy: 0.7881 - val_loss: 0.7808 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1014_58_54.727215/model-00008-0.53930-0.79940-0.78076-0.72000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.6340 - categorical_accuracy: 0.7759 - val_loss: 0.8201 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1014_58_54.727215/model-00009-0.59028-0.78281-0.82005-0.69000.h5\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.4796 - categorical_accuracy: 0.8227 - val_loss: 0.7353 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1014_58_54.727215/model-00010-0.47363-0.82655-0.73530-0.73000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.4943 - categorical_accuracy: 0.8278 - val_loss: 0.9125 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1014_58_54.727215/model-00011-0.48532-0.83183-0.91249-0.67000.h5\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.4503 - categorical_accuracy: 0.8440 - val_loss: 0.8636 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1014_58_54.727215/model-00012-0.45916-0.84012-0.86363-0.65000.h5\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.3954 - categorical_accuracy: 0.8532 - val_loss: 0.7231 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1014_58_54.727215/model-00013-0.39618-0.85370-0.72312-0.71000.h5\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.3488 - categorical_accuracy: 0.8797 - val_loss: 0.8063 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1014_58_54.727215/model-00014-0.34861-0.88084-0.80626-0.71000.h5\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.3832 - categorical_accuracy: 0.8771 - val_loss: 0.7177 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1014_58_54.727215/model-00015-0.34015-0.89065-0.71766-0.78000.h5\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 81s 2s/step - loss: 0.4383 - categorical_accuracy: 0.8514 - val_loss: 0.7792 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1014_58_54.727215/model-00016-0.40436-0.85596-0.77919-0.71000.h5\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.3833 - categorical_accuracy: 0.8679 - val_loss: 0.8576 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1014_58_54.727215/model-00017-0.37863-0.86878-0.85765-0.68000.h5\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 81s 2s/step - loss: 0.3477 - categorical_accuracy: 0.8734 - val_loss: 0.9249 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1014_58_54.727215/model-00018-0.33458-0.87858-0.92485-0.67000.h5\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.3142 - categorical_accuracy: 0.8812 - val_loss: 0.8949 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1014_58_54.727215/model-00019-0.31656-0.88235-0.89486-0.67000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 79s 2s/step - loss: 0.2828 - categorical_accuracy: 0.8985 - val_loss: 0.7767 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1014_58_54.727215/model-00020-0.26129-0.90422-0.77674-0.73000.h5\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.2384 - categorical_accuracy: 0.9224 - val_loss: 0.7202 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00021: saving model to model_init_2019-03-1014_58_54.727215/model-00021-0.22643-0.92459-0.72018-0.74000.h5\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.2658 - categorical_accuracy: 0.9102 - val_loss: 0.6669 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00022: saving model to model_init_2019-03-1014_58_54.727215/model-00022-0.25032-0.91629-0.66688-0.77000.h5\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 79s 2s/step - loss: 0.2445 - categorical_accuracy: 0.9110 - val_loss: 0.6555 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00023: saving model to model_init_2019-03-1014_58_54.727215/model-00023-0.24691-0.90875-0.65548-0.74000.h5\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.2852 - categorical_accuracy: 0.9054 - val_loss: 0.6369 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00024: saving model to model_init_2019-03-1014_58_54.727215/model-00024-0.23072-0.92383-0.63692-0.74000.h5\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.2676 - categorical_accuracy: 0.9040 - val_loss: 0.6750 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00025: saving model to model_init_2019-03-1014_58_54.727215/model-00025-0.24410-0.91403-0.67501-0.73000.h5\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.2402 - categorical_accuracy: 0.9224 - val_loss: 0.6721 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00026: saving model to model_init_2019-03-1014_58_54.727215/model-00026-0.22480-0.92459-0.67215-0.74000.h5\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.2305 - categorical_accuracy: 0.9154 - val_loss: 0.6782 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00027: saving model to model_init_2019-03-1014_58_54.727215/model-00027-0.21709-0.92157-0.67816-0.73000.h5\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 81s 2s/step - loss: 0.2300 - categorical_accuracy: 0.9143 - val_loss: 0.6735 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00028: saving model to model_init_2019-03-1014_58_54.727215/model-00028-0.22847-0.91629-0.67350-0.77000.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 79s 2s/step - loss: 0.2389 - categorical_accuracy: 0.9157 - val_loss: 0.6719 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00029: saving model to model_init_2019-03-1014_58_54.727215/model-00029-0.21272-0.92609-0.67185-0.76000.h5\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 80s 2s/step - loss: 0.2499 - categorical_accuracy: 0.9260 - val_loss: 0.6755 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00030: saving model to model_init_2019-03-1014_58_54.727215/model-00030-0.23651-0.92836-0.67552-0.77000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d6_model.count_params())\n",
    "history_model6=conv_3d6.train_model(conv_3d6_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "u8TDtt1LwZpD"
   },
   "source": [
    "###### For the above low memory foot print model the best validation accuracy of 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRyuVGhDwZpD"
   },
   "source": [
    "# Model 7 - reducing the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zYi6tNJowZpE"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D7(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02M-XsRVwZpM",
    "outputId": "937c8543-9a46-4df2-84a8-a5fd77510c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_61 (Conv3D)           (None, 16, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 16, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_41 (MaxPooling (None, 8, 60, 60, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_62 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 8, 60, 60, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 8, 60, 60, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_42 (MaxPooling (None, 4, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_63 (Conv3D)           (None, 4, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 4, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 4, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_43 (MaxPooling (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_64 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_44 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                401472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 504,709\n",
      "Trainable params: 503,973\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d7=ModelConv3D7()\n",
    "conv_3d7.initialize_path(project_folder)\n",
    "conv_3d7.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d7.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
    "conv_3d7_model=conv_3d7.define_model(dense_neurons=64,dropout=0.25)\n",
    "conv_3d7_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__uS11hKwZpQ",
    "outputId": "fe832163-baf0-4e48-9f49-d14fc18a1cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 504709\n",
      "Epoch 1/25\n",
      "34/34 [==============================] - 94s 3s/step - loss: 1.9593 - categorical_accuracy: 0.3330 - val_loss: 1.7143 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1014_21_25.841857/model-00001-1.96391-0.32881-1.71434-0.43000.h5\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - 83s 2s/step - loss: 1.2913 - categorical_accuracy: 0.5088 - val_loss: 1.2703 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1014_21_25.841857/model-00002-1.26297-0.51735-1.27031-0.60000.h5\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 1.1091 - categorical_accuracy: 0.5681 - val_loss: 0.8894 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1014_21_25.841857/model-00003-1.09435-0.57391-0.88939-0.70000.h5\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - 89s 3s/step - loss: 0.9943 - categorical_accuracy: 0.6159 - val_loss: 1.0272 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1014_21_25.841857/model-00004-0.97288-0.62293-1.02723-0.62000.h5\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.8651 - categorical_accuracy: 0.6641 - val_loss: 0.8217 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1014_21_25.841857/model-00005-0.86820-0.65988-0.82166-0.72000.h5\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.8126 - categorical_accuracy: 0.7141 - val_loss: 0.8461 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1014_21_25.841857/model-00006-0.77581-0.71946-0.84606-0.72000.h5\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - 89s 3s/step - loss: 0.6891 - categorical_accuracy: 0.7517 - val_loss: 0.8276 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1014_21_25.841857/model-00007-0.67156-0.74962-0.82756-0.70000.h5\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.7323 - categorical_accuracy: 0.7469 - val_loss: 0.7540 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1014_21_25.841857/model-00008-0.65875-0.75716-0.75403-0.72000.h5\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.6199 - categorical_accuracy: 0.7785 - val_loss: 1.2971 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1014_21_25.841857/model-00009-0.60802-0.78130-1.29712-0.69000.h5\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.6029 - categorical_accuracy: 0.7678 - val_loss: 0.9319 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1014_21_25.841857/model-00010-0.60691-0.76621-0.93185-0.68000.h5\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - 89s 3s/step - loss: 0.5616 - categorical_accuracy: 0.7962 - val_loss: 0.8556 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1014_21_25.841857/model-00011-0.54094-0.80769-0.85565-0.70000.h5\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.5021 - categorical_accuracy: 0.8238 - val_loss: 0.8616 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1014_21_25.841857/model-00012-0.49506-0.82353-0.86162-0.68000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.5554 - categorical_accuracy: 0.7943 - val_loss: 0.8370 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1014_21_25.841857/model-00013-0.49660-0.80995-0.83700-0.70000.h5\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.4491 - categorical_accuracy: 0.8274 - val_loss: 0.8091 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1014_21_25.841857/model-00014-0.44970-0.82730-0.80912-0.72000.h5\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - 89s 3s/step - loss: 0.4230 - categorical_accuracy: 0.8469 - val_loss: 0.7754 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1014_21_25.841857/model-00015-0.42840-0.84314-0.77535-0.74000.h5\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - 88s 3s/step - loss: 0.4198 - categorical_accuracy: 0.8455 - val_loss: 0.7715 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1014_21_25.841857/model-00016-0.41964-0.84992-0.77150-0.76000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - 87s 3s/step - loss: 0.3774 - categorical_accuracy: 0.8668 - val_loss: 0.7630 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1014_21_25.841857/model-00017-0.37937-0.86350-0.76299-0.77000.h5\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.4336 - categorical_accuracy: 0.8492 - val_loss: 0.7591 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1014_21_25.841857/model-00018-0.42083-0.85370-0.75913-0.77000.h5\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - 87s 3s/step - loss: 0.4700 - categorical_accuracy: 0.8484 - val_loss: 0.7614 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1014_21_25.841857/model-00019-0.39548-0.86953-0.76135-0.76000.h5\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.4922 - categorical_accuracy: 0.8363 - val_loss: 0.7546 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1014_21_25.841857/model-00020-0.41801-0.85294-0.75459-0.77000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5999999959603884e-06.\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.4301 - categorical_accuracy: 0.8469 - val_loss: 0.7569 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00021: saving model to model_init_2019-03-1014_21_25.841857/model-00021-0.41032-0.85143-0.75689-0.77000.h5\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - 85s 3s/step - loss: 0.4006 - categorical_accuracy: 0.8425 - val_loss: 0.7554 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00022: saving model to model_init_2019-03-1014_21_25.841857/model-00022-0.39977-0.84691-0.75538-0.76000.h5\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - 85s 3s/step - loss: 0.4267 - categorical_accuracy: 0.8550 - val_loss: 0.7526 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00023: saving model to model_init_2019-03-1014_21_25.841857/model-00023-0.36925-0.87632-0.75260-0.76000.h5\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.4250 - categorical_accuracy: 0.8506 - val_loss: 0.7522 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00024: saving model to model_init_2019-03-1014_21_25.841857/model-00024-0.39340-0.86350-0.75220-0.76000.h5\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - 86s 3s/step - loss: 0.4012 - categorical_accuracy: 0.8572 - val_loss: 0.7507 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00025: saving model to model_init_2019-03-1014_21_25.841857/model-00025-0.37967-0.86199-0.75072-0.76000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d7_model.count_params())\n",
    "history_model7=conv_3d7.train_model(conv_3d7_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0kcsNrCwZpZ"
   },
   "source": [
    "###### For the above low memory foot print model the best validation accuracy of 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymQqaWKKwZpZ"
   },
   "source": [
    "# Model 8 - reducing the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wVPxeM-_wZpa"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D8(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(8, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZmeqSBjwZpe",
    "outputId": "8e7c1b4e-574b-41f7-b09b-5ed5c79f2750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 16, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 8, 60, 60, 8)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 8, 60, 60, 16)     3472      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 60, 60, 16)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 60, 60, 16)     64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 4, 30, 30, 32)     4128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 15, 15, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                200768    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 230,949\n",
      "Trainable params: 230,453\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d8=ModelConv3D8()\n",
    "conv_3d8.initialize_path(project_folder)\n",
    "conv_3d8.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d8.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
    "conv_3d8_model=conv_3d8.define_model(dense_neurons=64,dropout=0.25)\n",
    "conv_3d8_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAMOdD3XwZph",
    "outputId": "373b0d5b-be6a-4a6b-814b-517c954228c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 230949\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - 220s 6s/step - loss: 1.9751 - categorical_accuracy: 0.2984 - val_loss: 1.4932 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1313_33_37.190087/model-00001-1.98941-0.29336-1.49321-0.42000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 88s 3s/step - loss: 1.4240 - categorical_accuracy: 0.4430 - val_loss: 1.1719 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1313_33_37.190087/model-00002-1.42481-0.44570-1.17192-0.54000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 1.2560 - categorical_accuracy: 0.4967 - val_loss: 1.1260 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1313_33_37.190087/model-00003-1.23420-0.50905-1.12604-0.56000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 1.1064 - categorical_accuracy: 0.5706 - val_loss: 1.0530 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1313_33_37.190087/model-00004-1.10417-0.57240-1.05301-0.62000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.9957 - categorical_accuracy: 0.6159 - val_loss: 1.0074 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1313_33_37.190087/model-00005-1.01027-0.60633-1.00736-0.61000.h5\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.9132 - categorical_accuracy: 0.6439 - val_loss: 0.9542 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1313_33_37.190087/model-00006-0.91047-0.64329-0.95417-0.65000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.8942 - categorical_accuracy: 0.6634 - val_loss: 0.9393 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1313_33_37.190087/model-00007-0.85051-0.67572-0.93927-0.66000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.8339 - categorical_accuracy: 0.6619 - val_loss: 0.9249 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1313_33_37.190087/model-00008-0.82348-0.67421-0.92494-0.65000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.8491 - categorical_accuracy: 0.6832 - val_loss: 0.9523 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1313_33_37.190087/model-00009-0.81928-0.68778-0.95233-0.64000.h5\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 89s 3s/step - loss: 0.7613 - categorical_accuracy: 0.7035 - val_loss: 0.9017 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1313_33_37.190087/model-00010-0.75755-0.70437-0.90171-0.69000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 92s 3s/step - loss: 0.7275 - categorical_accuracy: 0.7241 - val_loss: 0.9712 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1313_33_37.190087/model-00011-0.71167-0.73379-0.97125-0.67000.h5\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.6797 - categorical_accuracy: 0.7417 - val_loss: 0.9966 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1313_33_37.190087/model-00012-0.66884-0.74359-0.99656-0.65000.h5\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.6345 - categorical_accuracy: 0.7653 - val_loss: 0.9488 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1313_33_37.190087/model-00013-0.59910-0.77602-0.94882-0.68000.h5\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - 92s 3s/step - loss: 0.6072 - categorical_accuracy: 0.7653 - val_loss: 0.8965 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1313_33_37.190087/model-00014-0.60633-0.76772-0.89653-0.70000.h5\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - 92s 3s/step - loss: 0.5980 - categorical_accuracy: 0.7796 - val_loss: 0.7935 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1313_33_37.190087/model-00015-0.56709-0.78658-0.79354-0.73000.h5\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.6555 - categorical_accuracy: 0.7572 - val_loss: 0.8997 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1313_33_37.190087/model-00016-0.63651-0.76772-0.89971-0.72000.h5\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.6029 - categorical_accuracy: 0.7873 - val_loss: 0.7518 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1313_33_37.190087/model-00017-0.56391-0.79864-0.75180-0.75000.h5\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.5317 - categorical_accuracy: 0.8072 - val_loss: 0.7401 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1313_33_37.190087/model-00018-0.52178-0.81071-0.74012-0.74000.h5\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.5290 - categorical_accuracy: 0.8076 - val_loss: 0.8001 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1313_33_37.190087/model-00019-0.52923-0.80694-0.80010-0.73000.h5\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.4663 - categorical_accuracy: 0.8348 - val_loss: 0.7581 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1313_33_37.190087/model-00020-0.44058-0.84314-0.75806-0.76000.h5\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - 92s 3s/step - loss: 0.4716 - categorical_accuracy: 0.8322 - val_loss: 0.7805 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00021: saving model to model_init_2019-03-1313_33_37.190087/model-00021-0.47827-0.82805-0.78050-0.74000.h5\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.4879 - categorical_accuracy: 0.8216 - val_loss: 0.8165 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00022: saving model to model_init_2019-03-1313_33_37.190087/model-00022-0.47862-0.82127-0.81652-0.73000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.4559 - categorical_accuracy: 0.8403 - val_loss: 0.7845 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00023: saving model to model_init_2019-03-1313_33_37.190087/model-00023-0.43954-0.84465-0.78446-0.72000.h5\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.4735 - categorical_accuracy: 0.8120 - val_loss: 0.7462 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00024: saving model to model_init_2019-03-1313_33_37.190087/model-00024-0.46460-0.81976-0.74616-0.73000.h5\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.4274 - categorical_accuracy: 0.8572 - val_loss: 0.7319 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00025: saving model to model_init_2019-03-1313_33_37.190087/model-00025-0.38593-0.87029-0.73195-0.77000.h5\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.4029 - categorical_accuracy: 0.8576 - val_loss: 0.7216 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00026: saving model to model_init_2019-03-1313_33_37.190087/model-00026-0.39121-0.86652-0.72163-0.78000.h5\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.4319 - categorical_accuracy: 0.8477 - val_loss: 0.7274 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00027: saving model to model_init_2019-03-1313_33_37.190087/model-00027-0.39914-0.86048-0.72740-0.76000.h5\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.4081 - categorical_accuracy: 0.8587 - val_loss: 0.7451 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00028: saving model to model_init_2019-03-1313_33_37.190087/model-00028-0.39095-0.86350-0.74510-0.76000.h5\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - 91s 3s/step - loss: 0.3960 - categorical_accuracy: 0.8536 - val_loss: 0.7319 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00029: saving model to model_init_2019-03-1313_33_37.190087/model-00029-0.39294-0.85822-0.73193-0.77000.h5\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.3807 - categorical_accuracy: 0.8745 - val_loss: 0.7095 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00030: saving model to model_init_2019-03-1313_33_37.190087/model-00030-0.36469-0.87557-0.70946-0.77000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d8_model.count_params())\n",
    "history_model8=conv_3d8.train_model(conv_3d8_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "cK8kmbObwZpn"
   },
   "source": [
    "###### For the above low memory foot print model the best validation accuracy of 78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4ij_vFOwZpo"
   },
   "source": [
    "# Model 9 - CNN- LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_J6xYvsBwZpo"
   },
   "outputs": [],
   "source": [
    "class RNNCNN1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        #model.add(TimeDistributed(Conv2D(512, (2, 2) , padding='valid', activation='relu')))\n",
    "       # model.add(TimeDistributed(BatchNormalization()))\n",
    "       # model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4m8EQyewZpr",
    "outputId": "0951e78e-82ec-4fc5-b8f4-046a5b67bede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_33 (TimeDis (None, 18, 120, 120, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, 18, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, 18, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, 18, 60, 60, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_37 (TimeDis (None, 18, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_38 (TimeDis (None, 18, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_39 (TimeDis (None, 18, 30, 30, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_40 (TimeDis (None, 18, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_41 (TimeDis (None, 18, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_42 (TimeDis (None, 18, 15, 15, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_43 (TimeDis (None, 18, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_44 (TimeDis (None, 18, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_45 (TimeDis (None, 18, 7, 7, 256)     295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_46 (TimeDis (None, 18, 7, 7, 256)     1024      \n",
      "_________________________________________________________________\n",
      "time_distributed_47 (TimeDis (None, 18, 3, 3, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_48 (TimeDis (None, 18, 2304)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               1245696   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,657,445\n",
      "Trainable params: 1,656,453\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn1=RNNCNN1()\n",
    "rnn_cnn1.initialize_path(project_folder)\n",
    "rnn_cnn1.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
    "rnn_cnn1_model=rnn_cnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPtayj1_wZpu",
    "outputId": "fdd3500e-b8a4-4289-d901-cc0d490093e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1657445\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - 185s 5s/step - loss: 1.4812 - categorical_accuracy: 0.3587 - val_loss: 1.3154 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1105_50_41.913699/model-00001-1.47905-0.36350-1.31544-0.47000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 97s 3s/step - loss: 1.1502 - categorical_accuracy: 0.5394 - val_loss: 1.0631 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1105_50_41.913699/model-00002-1.15262-0.53620-1.06311-0.47000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 101s 3s/step - loss: 1.1370 - categorical_accuracy: 0.5361 - val_loss: 1.9629 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1105_50_41.913699/model-00003-1.13109-0.53695-1.96287-0.26000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 100s 3s/step - loss: 0.9926 - categorical_accuracy: 0.6155 - val_loss: 1.9898 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1105_50_41.913699/model-00004-0.97437-0.61840-1.98977-0.31000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 100s 3s/step - loss: 0.8681 - categorical_accuracy: 0.6531 - val_loss: 1.2251 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1105_50_41.913699/model-00005-0.87046-0.64857-1.22508-0.49000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.7565 - categorical_accuracy: 0.7009 - val_loss: 1.1085 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1105_50_41.913699/model-00006-0.73786-0.70588-1.10852-0.54000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.6277 - categorical_accuracy: 0.7542 - val_loss: 0.6773 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1105_50_41.913699/model-00007-0.61060-0.77300-0.67726-0.72000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.4904 - categorical_accuracy: 0.8120 - val_loss: 0.6652 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1105_50_41.913699/model-00008-0.49187-0.81146-0.66520-0.74000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.4368 - categorical_accuracy: 0.8477 - val_loss: 0.5897 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1105_50_41.913699/model-00009-0.44157-0.84389-0.58974-0.73000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.3936 - categorical_accuracy: 0.8536 - val_loss: 0.6429 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1105_50_41.913699/model-00010-0.38632-0.85822-0.64294-0.71000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.3664 - categorical_accuracy: 0.8705 - val_loss: 0.6210 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1105_50_41.913699/model-00011-0.37060-0.86727-0.62103-0.75000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.3322 - categorical_accuracy: 0.8885 - val_loss: 0.6073 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1105_50_41.913699/model-00012-0.33066-0.88989-0.60728-0.74000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.3482 - categorical_accuracy: 0.8775 - val_loss: 0.7360 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1105_50_41.913699/model-00013-0.31848-0.88688-0.73596-0.71000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.2648 - categorical_accuracy: 0.9198 - val_loss: 0.6653 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1105_50_41.913699/model-00014-0.25089-0.92609-0.66530-0.73000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.2833 - categorical_accuracy: 0.9106 - val_loss: 0.6429 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1105_50_41.913699/model-00015-0.28045-0.91252-0.64286-0.74000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.2534 - categorical_accuracy: 0.9161 - val_loss: 0.6382 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1105_50_41.913699/model-00016-0.25469-0.91403-0.63821-0.73000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.2303 - categorical_accuracy: 0.9253 - val_loss: 0.6131 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1105_50_41.913699/model-00017-0.23078-0.92760-0.61311-0.75000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.2564 - categorical_accuracy: 0.9135 - val_loss: 0.6104 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1105_50_41.913699/model-00018-0.24279-0.92383-0.61041-0.72000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.2112 - categorical_accuracy: 0.9308 - val_loss: 0.6079 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1105_50_41.913699/model-00019-0.21417-0.92911-0.60786-0.72000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.2268 - categorical_accuracy: 0.9290 - val_loss: 0.6128 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1105_50_41.913699/model-00020-0.22072-0.93137-0.61277-0.72000.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn1_model.count_params())\n",
    "history_model9=rnn_cnn1.train_model(rnn_cnn1_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Oj1hMilHwZpx"
   },
   "source": [
    "##### For CNN - LSTM model we get a best validation accuracy of 75%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gesture_Recognition_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
